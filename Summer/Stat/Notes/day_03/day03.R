# STAT 6430
# Day 3 -- R
# 
# Today we consider statistical concepts, including an introduction
# to simple linear regression.
#

#
## Sampling distributions
#
# We need some data:
#
d03a <- read.table("d03a.txt",header=T,sep="")[,1]
#
# Suppose that d1 is regarded as a population 
# and that we can only sample from this.  We would
# like to estimate the mean transaction time for the 
# population. 
# 
# We can start estimating the mean with a sample:
#
x1 <- sample(d03a, size=1, replace=T)
x1
#
# Team Questions
# 1. Is x1 the mean?  If not, then
# how close is x1 to the mean?
#
# Let's take a bigger sample, of size 40, and
# compute the mean of this sample:
#
x2 <- sample(d03a, size=40, replace=T)
x2
mean(x2)
#
# Team Questions
# 1. Is mean(x2) the population mean?  If not, then
#    how close is mean(x2) to the population mean? 
# 2. Suppose we took lots of samples, and computed the
#    mean for each.  What is the average of these values?
#    What else do we know about them?
#
#
# 
#
#
#
means <- rep(0,1000) # Initialize vector of means
for (i in 1:1000) {
  means[i] <- mean(sample(d03a, size=30, replace=T))
}
hist(means)

#
# The means appear to have a distribution that is approximately
# normal.  Thus about 95% of mean values should be within 2
# standard deviations of the population mean.

m <- mean(means)  # Estimate for population mean
s <- sd(means) # Estimate for standard deviation of means
c(m - 1.96*s, m + 1.96*s)
quantile(means,c(.025, .975))

# Team Question:
# Now suppose that we only have one sample.  How can we proceed?

#
#
## Simple linear regression
#
# Start by importing data set "brainhead-train.csv" that
# has 200 measurements of head size (cm^3) and brain
# weight (grams).  
#
bh.train = read.csv("brainhead-train.csv", header = TRUE)
#
# It is always a good idea to graph your data.  There
# are lots of options, but since we only care about
# head size and brain weight, a simple scatter plot will
# do.
plot(bh.train,pch=20,cex=.2)
#
# We can fit a line to the data x = HeadSize and
# y = BrainWeight with the following:
#
brain.lm <- lm(BrainWeight ~ HeadSize, data=bh.train)
#
# A summary of the values generated by lm:
#
summary(brain.lm)
#
# We can add a plot of the regression line to 
# the scatterplot.
#
abline(brain.lm)
#
# The various components in brain.lm:
#
names(brain.lm)
brain.lm$residuals
#
# Plotting the residuals can help reveal any model
# departures
#.
plot(brain.lm$residuals,pch=20,cex=.2) # Normal dist?
#
# The MSE is the average of the squared residuals:
#
mse <- mean(brain.lm$residuals^2)
#
# Note that this is slightly different than the usual 
# definition of MSE from ANOVA, where one divides by
# n-2 instead of n.  
#
# We will use the MSE to give a measure of how well 
# the model predicts values of the y (dependent) variable.
# The smaller the MSE, the better.

## Polynomials
#
# Sometimes a polynomial better fits data than a line.
# Here's the plot of the brain data again:
#
plot(bh.train,pch=20,cex=.2)
#
# The "poly" function can be used to fit a polynomial
# to data.  Here is a degree 2 polynomial:
#
brain.lm2 <- lm(BrainWeight ~ poly(HeadSize,2), data=bh.train)
#
# A summary of the values generated by lm:
#
summary(brain.lm2)
# 
# A comparison of the MSE's for linear and quadratic models:
#
mse <- mean(brain.lm$residuals^2)
mse
mse2 <- mean(brain.lm2$residuals^2)
mse2

# Team Question
#  1. Do higher degree polynomials do better?
#

# Here's a different data set, related to the price of 
# diamonds (c = carets, p = price($)):
#
d03b <- read.csv("d03b.csv",header=T)
plot(d03b,pch=20,cex=.2)

# Team Questions:
# 1. Find a model (linear or polynomial) that seems to give
#    a good fit to the data.
# 2. Find a model for the subset of diamonds the weight less 
#    than one caret.
#

## Transformations
#
# Besides polynomials, it is also common to try taking the 
# logarithm of either variable to improve the model.  Here's
# a plot of the diamond data, using log(p) in place of p:

plot(log(p)~c, data=d03b,pch=20,cex=.2)
d03b.lmlog <- lm(log(p)~c, data=d03b)
summary(d03b.lmlog)
mean(d03b.lmlog$residuals^2)

# Team Questions:
# 1. Try out models fitted to the diamond data that use log
#    transformations on one or both variables.  Are any of 
#    these better than the earlier models?

d03c <- read.csv("d03c.csv",header=T)
plot(d03c,pch=20,cex=.2)

# Team Questions:
# 1. For the set d03c, find a model that seems to best fit
#    the data.
# 2. Select at random 100 of the records in d03c, and use 
#    that subset to repeat #1.  Does it make a difference?



