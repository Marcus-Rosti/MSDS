# Team Assignment 3
#
#
# A quick review of some multiple regression basics 
# in R.  This isn't exhaustive, just a starting point.
#
#
setwd("~/MSDS/Fall/Stat_6021/Team_Assign/team3")

dat <- read.csv("TA03train.csv", header = TRUE)
View(dat)  # Pops up table version of dat.
#
# A matrix plot of the data
#
plot(dat,pch=20,cex=.2)  # Matrix plot in this case
#
dat01.lm <- lm(y ~ ., data=dat)
#
# A summary of the values generated by lm:
#
summary(dat01.lm)
#
# Here's what we get from using just x3 and x5:
#
dat02.lm <- lm(y ~ x3+x5, data=dat)
summary(dat02.lm)
#
# The "leaps" package makes available the "regsubsets" function that
# will give the best model for different numbers of variables:
install.packages("leaps")
library(leaps)

# regsubsets gives each size model that has the smallest RSS
# residual sum of squares (aka SS_Res)
regsub.dat01 <- regsubsets(y~., data=dat)
summary(regsub.dat01)

# Here are the SS_Res's for each model:
summary(regsub.dat01)$rss

# There are other criteria that are used for model selection
#
summary(regsub.dat01)$rsq    # R^2
summary(regsub.dat01)$adjr2  # adjusted R^2
summary(regsub.dat01)$cp     # Mallow's C_p (smaller is better)
summary(regsub.dat01)$bic    # Bayes Information Criterion (negative OK)

names(summary(regsub.dat01))

# The regsubsets function can also perform forward and backward
# stepwise selection:
regsub.dat01for <- regsubsets(y~., data=dat, method="forward")
summary(regsub.dat01for)

regsub.dat01back <- regsubsets(y~., data=dat, method="backward")
summary(regsub.dat01back)

# Predictions: The "predict" function will combine the model of
# your choosing with a set of explanatory variables to give
# corresponding predicted y-values.

datpred <- read.csv("TA03predict.csv", header = TRUE)
mypreds <- as.vector(predict(dat01.lm, newdata=datpred))
mypreds[1:20]  # 20 of 400 predicted values

# Partial F-Test: This test's a full regression model containing
# all variables against a partial model containing a subset of 
# variables.  The null hypothesis is that both models fit the 
# data equally well, the alternative is that the full model is
# superior.  

dat.lmpart <- lm(y ~ x3+x5, data=dat)
dat.lmfull <- lm(y ~ ., data=dat)
anova(dat.lmpart,dat.lmfull)

# Here p < 2.2e-16, which implies that the full model is
# superior to the partial model.  

#
## ANOVA Table
#
# Gives additional SS_R for each variable when those above 
# are already in the model.

anova(dat.lmfull) 
anova(dat.lmpart)

#
## Multicollinearity
#
# We can check for multicollinearity by using the
# variance inflation factors, given by the "vif"
# function.  (This if for the quantitative variables.)
# Note: We use the "car" package version of "vif"
# which is easy to implement and has simple output.

install.packages("car")
library(car)
vif(dat.lmfull) # < 5 is generally fine.
############################################################################################################
#
# 
#
#
############################################################################################################
calc.mse <- function(lm) {
  return (mean(lm$residuals^2))
}

loocv=function(fit){
  h=lm.influence(fit)$h
  mean((residuals(fit)/(1-h))^2)
}

